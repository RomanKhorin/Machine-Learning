{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №1\n",
    "Хорин Роман ИАД-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо проверить следующую гипотезу: в текстах разных стилей частоты речи имеют разные характеры распределений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymystem3 as stm\n",
    "import pymorphy2\n",
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from scipy import stats\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основные методы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном разделе представлены основные методы, использующиеся в рамках данного домашнего задания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'ADJF' - прилагательное (полное)\n",
    "# 'ADJS' - прилагательное (краткое)\n",
    "# 'ADVB' - наречие\n",
    "# 'COMP' - компаратив (сравнение)\n",
    "# 'CONJ' - союз\n",
    "# 'GRND' - деепричастие\n",
    "# 'INFN' - инфинитив\n",
    "# 'INTJ' - междометие\n",
    "# 'NOUN' - существительное\n",
    "# 'NPRO' - местомение-существительное\n",
    "# 'NUMR' - числительное\n",
    "# 'PRCL' - частица\n",
    "# 'PRED' - предикатив\n",
    "# 'PREP' - предлог\n",
    "# 'PRTF' - причастие (полное)\n",
    "# 'PRTS' - причастие (короткое)\n",
    "# 'VERB' - глагол"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Метод формирует список частей речи, имеющихся в тексте\"\"\"\n",
    "\n",
    "def Gain_POSes(types):\n",
    "    parts_of_speech = []\n",
    "    for t in types:\n",
    "        try:\n",
    "            gram_info = morph.parse(t)[0]\n",
    "            POS = gram_info.tag.POS # определяем часть речи слова\n",
    "            if POS in parts_of_speech:\n",
    "                continue\n",
    "            else:\n",
    "                parts_of_speech.append(POS) # добавляем часть речи в список, если она не встречалась в нем раньше\n",
    "        except:\n",
    "            pass\n",
    "    try:\n",
    "        parts_of_speech.remove(None) # удаляем None\n",
    "    except:\n",
    "        pass\n",
    "    return parts_of_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Метод считает количество слов определенной части речи в тексте\"\"\"\n",
    "\n",
    "def Count_part_of_the_speech(text_types, part_name):\n",
    "    part_dict = nltk.FreqDist()\n",
    "    for t in text_types:\n",
    "        try:\n",
    "            gram_info = morph.parse(t)[0] # парсим слово\n",
    "            if part_name == gram_info.tag.POS: # проверям на равенство части речи         \n",
    "                part_dict[t] += text_types[t] # добавляем в словарь\n",
    "        except:\n",
    "            pass\n",
    "    return sum(part_dict.values()) # возращаем количество слов, соответствующее части речи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Метод формирует частотный словарь частей речи текста\"\"\"\n",
    "\n",
    "def Generate_pos_freq_dic(types, parts_of_speech):\n",
    "    freq = nltk.FreqDist()\n",
    "    for pos in parts_of_speech: \n",
    "        # для каждой части речи определяем количество слов, ей соответствующее, и добавляем количество в частотный словарь\n",
    "        freq[pos] = Count_part_of_the_speech(types, pos) \n",
    "    return freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основная часть работы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной части работы проводится основная работа с текстами (их очистка и токенизация), составляются частотный словари (часть речи-количество слов) для каждого из текстов, а также считается коэффициент корреляции Спирмена между составленными частотными словарями частей речи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Считываем коллекцию текстов двух разных стилей, считаем количество токеном и типов в каждой коллекции.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publicistic text with 5944 tokens and 3003 types. \n",
      "\n",
      "Artistic text with 5848 tokens and 3090 types.\n"
     ]
    }
   ],
   "source": [
    "infile_publ = 'publicism.txt'\n",
    "infile_art = 'art.txt'\n",
    "\n",
    "# исключающая пунктуация, цифры и знаки\n",
    "exclude = set(punctuation + '0123456789' + u'-' + u'—'+u'–—'+u'«»'+'\\n'+ u'\\ufeff')\n",
    "\n",
    "# считываем тексты\n",
    "text_publ = open(infile_publ, 'r', encoding='utf-8').read()\n",
    "text_art = open(infile_art, 'r', encoding='utf-8').read()\n",
    "\n",
    "\n",
    "# проводим предварительную очистку текстов\n",
    "text_publ = ''.join(ch for ch in text_publ if ch not in exclude)\n",
    "text_art = ''.join(ch for ch in text_art if ch not in exclude)\n",
    "\n",
    "# Считаем количество токенов и типов в текстах\n",
    "# токен - слово + его место в тексте\n",
    "# тип - словоформа\n",
    "\n",
    "tokens_publ = WhitespaceTokenizer().tokenize(text_publ.lower())\n",
    "types_publ = nltk.FreqDist(tokens_publ)\n",
    "print('Publicistic text with %d tokens and %d types. \\n'%(len(tokens_publ), len(types_publ)))\n",
    "\n",
    "tokens_art = WhitespaceTokenizer().tokenize(text_art.lower())\n",
    "types_art = nltk.FreqDist(tokens_art)\n",
    "print('Artistic text with %d tokens and %d types.'%(len(tokens_art), len(types_art)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. При помощи nltk.FreqDist() и морфологического процессора pymorphy2 составляем частоные словари: часть речи - количество слов, к ней относящихся."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формируем список частей речи, используя метод Gain_POSes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOUN',\n",
       " 'ADJF',\n",
       " 'ADJS',\n",
       " 'VERB',\n",
       " 'NPRO',\n",
       " 'PRED',\n",
       " 'COMP',\n",
       " 'CONJ',\n",
       " 'NUMR',\n",
       " 'PREP',\n",
       " 'PRTF',\n",
       " 'INTJ',\n",
       " 'ADVB',\n",
       " 'PRTS',\n",
       " 'GRND',\n",
       " 'INFN',\n",
       " 'PRCL']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts_of_speech_publ = Gain_POSes(types_publ)\n",
    "parts_of_speech_art = Gain_POSes(types_art)\n",
    "\n",
    "# общий список частей речи представляет из себя объединение множеств частей речи из обоих текстов\n",
    "parts_of_speech = list(set.union(set(parts_of_speech_publ), set(parts_of_speech_art)))\n",
    "parts_of_speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формируем частотные словари частей речи для обоих текстов, используя метод Generate_pos_freq_dic()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 36.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pos_freq_dic_publ = Generate_pos_freq_dic(types_publ, parts_of_speech)\n",
    "pos_freq_dic_art = Generate_pos_freq_dic(types_art, parts_of_speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Частотный словарь частей речи для художественного текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artistic text POS frequency dictionary:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({'ADJF': 936,\n",
       "          'ADJS': 80,\n",
       "          'ADVB': 353,\n",
       "          'COMP': 13,\n",
       "          'CONJ': 540,\n",
       "          'GRND': 74,\n",
       "          'INFN': 157,\n",
       "          'INTJ': 6,\n",
       "          'NOUN': 1790,\n",
       "          'NPRO': 283,\n",
       "          'NUMR': 19,\n",
       "          'PRCL': 298,\n",
       "          'PRED': 23,\n",
       "          'PREP': 613,\n",
       "          'PRTF': 152,\n",
       "          'PRTS': 38,\n",
       "          'VERB': 463})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Artistic text POS frequency dictionary:')\n",
    "pos_freq_dic_art"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Частотный словарь частей речи для публицистического текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publicistic text POS frequency dictionary:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({'ADJF': 744,\n",
       "          'ADJS': 33,\n",
       "          'ADVB': 248,\n",
       "          'COMP': 19,\n",
       "          'CONJ': 461,\n",
       "          'GRND': 14,\n",
       "          'INFN': 143,\n",
       "          'INTJ': 9,\n",
       "          'NOUN': 2241,\n",
       "          'NPRO': 201,\n",
       "          'NUMR': 34,\n",
       "          'PRCL': 175,\n",
       "          'PRED': 21,\n",
       "          'PREP': 778,\n",
       "          'PRTF': 87,\n",
       "          'PRTS': 63,\n",
       "          'VERB': 587})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Publicistic text POS frequency dictionary:')\n",
    "pos_freq_dic_publ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Считаем коэффициент корреляции Спирмена между сформированными частотными словарями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation between two texts: 0.950980392157\n"
     ]
    }
   ],
   "source": [
    "corrcoef = stats.spearmanr(list(pos_freq_dic_art.values()), list(pos_freq_dic_publ.values()))[0]\n",
    "print('Spearman correlation between two texts:',corrcoef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коэффициент корреляции Спирмена оказался 0,95. Это означает, что по частотной составляющей тексты вполне схожи друг с другом, но имеют некоторые отличия. Так в художественном тексте больше кратких и полных прилагательных, а также полных причастий, чем в публицистическом. В публицистическом тексте же больше глаголов, существительных и предлогов, чем в художественном."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
